# from robots.franky_env import FrankyEnv
# from controllers.gello_env import GelloEnv
# from controllers.spacemouse_env import SpaceMouseEnv
# from cameras.realsense_env import RealSenseEnv
# from cameras.usb_env import USBEnv

# from common.constants import ActionSpace
# import time
# from pathlib import Path
# import logging
# from systems.robot_policy_utils import WebsocketClientPolicy
# from systems.tcp_client import TCPClientPolicy
# import cv2
# import numpy as np
# from cameras.camera_param import CameraParam
# from robots.robot_param import RobotParam
# import math
# import threading

# class RobotPolicySystem:
#     def __init__(self, action_space: ActionSpace = ActionSpace.JOINT_ANGLES, ip: str = "10.21.40.5", port: str = "8003", 
#                  action_only_mode: bool = False, calibration: bool=True):
#         # åˆå§‹åŒ–æœºå™¨äººç¯å¢ƒ
#         self.action_space = action_space
#         self.action_only_mode = action_only_mode

#         self.robot_env = FrankyEnv(action_space=action_space, inference_mode=True, robot_param=RobotParam(np.array([ 0.0, 0.0, -math.pi / 2]), np.array([ 0.53433071, 0.52905707, 0.00440881])))
#         if self.action_space not in [ActionSpace.EEF_VELOCITY, ActionSpace.JOINT_ANGLES]:
#             raise NotImplementedError(f"Action space '{self.action_space}' is not supported.")
#         logging.info(f"Trying to connect to policy server at {ip}:{port}...")
#         # self.client = WebsocketClientPolicy(
#         #     host= ip,
#         #     port= port
#         # )
#         self.client = TCPClientPolicy(
#             host= ip,
#             port= port
#         )
#         logging.info(f"Connected to policy server at {ip}:{port}.")
        
#         self.main_camera = RealSenseEnv(camera_name="main_image", serial_number="339322073638", width=1280, height=720,
#                                         camera_param=CameraParam(intrinsic_matrix = np.array([[908.1308, 0, 655.7268], [0, 910.0818, 395.8856], [0, 0, 1]], dtype=np.float32),
#                                                                  distortion_coeffs = np.array([0.1068, -0.2123, -0.0092, 0.0000, 0.0000], dtype=np.float32)))
#         self.wrist_camera = RealSenseEnv(camera_name="wrist_image", serial_number="342222072092", width=1280, height=720)
        
#         # åªåœ¨éaction_onlyæ¨¡å¼ä¸‹åˆå§‹åŒ–top_camera
        
#         self.top_camera = USBEnv(camera_name="top_image", serial_number="12", width=1920, height=1080, exposure=100,
#                         camera_param=CameraParam(np.array([[1158.0, 0, 999.9484], [0, 1159.9, 584.2338], [0, 0, 1]], dtype=np.float32), np.array([0.0412, -0.0509, 0.0000, 0.0000, 0.0000], dtype=np.float32))
#                     )
#         if calibration:
#             self.main_camera.calib_camera()
#             self.top_camera.calib_camera()

#         self.gripper_status = {
#             "current_state": 0,
#             "target_state": 0 
#         }
#         self.stop_evaluation = threading.Event()
#         self.all_action_and_traj = []
#         self.all_action_and_traj_lock = threading.Lock()

#     def reset_for_collection(self):
#         """é‡ç½®æœºå™¨äººåˆ°éšæœºä½ç½®ï¼Œç”¨äºæ•°æ®æ”¶é›†"""
#         self.robot_env.reset()
#         action = np.array([0,0,-0.05,0,0,0])
#         self.robot_env.step(action, asynchronous=False)
#         return True



#     def run(self, show_image: bool = False, task_name: str = "default_task"):
#         self.main_camera.start_monitoring()
#         self.wrist_camera.start_monitoring()
#         self.top_camera.start_monitoring()
#         # self.robot_env.step(np.array([0.01,0.01, -0.02,0,0,0]), asynchronous=False)

#         self.gripper_status = {
#             "current_state": 0,
#             "target_state": 0 
#         }
#         self.stop_evaluation.clear()
#         all_action_and_traj = []
#         while not self.stop_evaluation.is_set():
#             main_image = self.main_camera.get_latest_frame()['bgr']
#             wrist_image = self.wrist_camera.get_latest_frame()['bgr']
#             top_image = self.top_camera.get_latest_frame()['bgr']

#             if main_image is None or wrist_image is None:
#                 time.sleep(0.05)
#                 continue
                
#             joint_angles = self.robot_env.get_position(action_space=ActionSpace.JOINT_ANGLES)
#             gripper_width = self.robot_env.get_gripper_width()
#             eef_pose = self.robot_env.get_position(action_space=ActionSpace.EEF_POSE)
#             state = np.concatenate([eef_pose, [gripper_width]])
#             # æ ¹æ®æ¨¡å¼é€‰æ‹©ä¸åŒçš„å¤„ç†é€»è¾‘
#             if self.action_only_mode:
#                 state_trajectory = eef_pose[:3]
#                 element = {
#                     "observation/image": main_image,
#                     "observation/wrist_image": wrist_image,
#                     "observation/state": state,
#                     "prompt": task_name,
#                 }
#             else:
#                 state_trajectory = self.robot_env.robot_param.transform_to_world(np.array([eef_pose[:3]]))[0]
#                 element = {
#                     "observation/image": main_image,
#                     "observation/wrist_image": wrist_image,
#                     "observation/state": state,
#                     "qpos": joint_angles.tolist(),
#                     "observation/state_trajectory": state_trajectory,
#                     "prompt": task_name,
#                 }

#             inference_results = self.client.infer(element)
#             actions_chunk = np.array(inference_results["actions"])
            
#             if not self.action_only_mode:
#                 trajectory_chunk = np.array(inference_results["trajectory"])
#             all_action_and_traj.append({
#                 'actions': actions_chunk.tolist(),
#                 'trajectory': trajectory_chunk.tolist() if not self.action_only_mode else None,
#                 'timestamp': time.time(),
#                 'state': state.tolist(),
#                 'state_trajectory': state_trajectory.tolist() if not self.action_only_mode else None
#             }.copy())
#             with self.all_action_and_traj_lock:
#                 self.all_action_and_traj = all_action_and_traj

#             cnt = 0
            
#             if show_image:
#                 draw_main_image = main_image.copy()
                
                
#                 draw_top_image = top_image.copy()
                
#                 action_trajectory = 0.1 * np.cumsum(actions_chunk,axis=0)
#                 action_trajectory_in_world = self.robot_env.robot_param.transform_to_world(action_trajectory[:,:3] + eef_pose[:3])
#                 if not self.action_only_mode:
#                     draw_main_image = self.main_camera.camera_param.draw_trajectory_on_image(draw_main_image, trajectory_chunk)
#                     draw_top_image = self.top_camera.camera_param.draw_trajectory_on_image(draw_top_image, trajectory_chunk)

#                 draw_main_image = self.main_camera.camera_param.draw_trajectory_on_image(draw_main_image, action_trajectory_in_world)
#                 draw_top_image = self.top_camera.camera_param.draw_trajectory_on_image(draw_top_image, action_trajectory_in_world)
#                 cv2.imshow("Top Camera", draw_top_image)
                
#                 cv2.imshow("Main Camera", draw_main_image)
#                 cv2.imshow("Wrist Camera", wrist_image)
#                 cv2.waitKey(1)

#             for action in actions_chunk:
#                 self.robot_env.step(action[:-1], asynchronous=True)
#                 time.sleep(0.1)

#                 cnt += 1
#                 gripper_action = action[-1]
                

#                 if gripper_action > 0.95:
#                     self.gripper_status["target_state"] = 1
#                 elif gripper_action < -0.95:
#                     self.gripper_status["target_state"] = -1
                
#                 if self.gripper_status["current_state"] != self.gripper_status["target_state"]:
#                     if self.gripper_status["target_state"] == -1:
#                         self.robot_env.open_gripper(asynchronous=True)
#                     else:
#                         self.robot_env.close_gripper(asynchronous=True)
#                     self.gripper_status["current_state"] = self.gripper_status["target_state"]
                
#                 max_cnt = 10

#                 if cnt == max_cnt:
#                     self.robot_env.step(np.array([0,0,0,0,0,0]), asynchronous=False)
#                     break
#     def stop(self):
#         self.stop_evaluation.set()
#         time.sleep(0.5)

#         self.robot_env.stop_saving_state()
#         logging.info("Robot policy system stopped.")

# # if __name__ == "__main__":
# #     logging.basicConfig(
# #             level=logging.INFO,
# #             format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
# #             handlers=[
# #                 logging.StreamHandler(),  # è¾“å‡ºåˆ°æ§åˆ¶å°
# #             ]
# #         )
# #     # ä½¿ç”¨ action_only_mode å‚æ•°æ§åˆ¶æ¨¡å¼
# #     # action_only_mode=True å¯¹åº”åŸæ¥çš„ robot_policy_action_only_system
# #     # action_only_mode=False å¯¹åº”åŸæ¥çš„ robot_policy_system
# #     system = RobotPolicySystem(action_space=ActionSpace.EEF_VELOCITY, action_only_mode=True, prompt="pick up the water bottle",
# #                               camera_calib_file="/home/dell/maple_control/data/20250829_fruits_and_tray/20250829_173310")
# #     system.run(show_image=True)

# if __name__ == "__main__":
#     logging.basicConfig(
#             level=logging.INFO,
#             format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
#             handlers=[
#                 logging.StreamHandler(),
#             ]
#         )
    
#     # === ä¿®æ”¹ç‚¹ 1: åˆå§‹åŒ–å‚æ•°ä¿®æ­£ ===
#     # 1. action_space æ”¹ä¸º JOINT_ANGLES (å› ä¸º RDT è¾“å‡ºçš„æ˜¯ 7 ç»´å…³èŠ‚è§’)
#     # 2. å»æ‰ prompt å’Œ camera_calib_file (åŸç±»å®šä¹‰ä¸­æ²¡æœ‰è¿™äº›å‚æ•°)
#     # 3. ip æ”¹ä¸ºä½ çš„æœåŠ¡å™¨ IP (å¦‚æœæ˜¯åŒä¸€å°æœºå™¨ç”¨ 127.0.0.1)
#     system = RobotPolicySystem(
#         action_space=ActionSpace.JOINT_ANGLES, 
#         ip="127.0.0.1", 
#         port=6000,
#         action_only_mode=False
#     )
    
#     # === ä¿®æ”¹ç‚¹ 2: åœ¨ run ä¸­ä¼ å…¥æŒ‡ä»¤ ===
#     # task_name å¯¹åº”åŸæ¥çš„ prompt
#     system.run(show_image=True, task_name="pick up the water bottle")


# import sys
# import os
# from robots.franky_env import FrankyEnv
# from common.constants import ActionSpace
# import time
# import logging
# from systems.tcp_client import TCPClientPolicy 
# import cv2
# import numpy as np
# from robots.robot_param import RobotParam
# import math
# import threading

# class RobotPolicySystem:
#     def __init__(self, action_space: ActionSpace = ActionSpace.JOINT_ANGLES, ip: str = "127.0.0.1", port: int = 6000, 
#                  action_only_mode: bool = False, calibration: bool=True):
#         self.action_space = action_space
#         self.action_only_mode = action_only_mode

#         # åˆå§‹åŒ– Franka æœºå™¨äºº
#         # inference_mode=True é€šå¸¸æ„å‘³ç€æ›´çµæ•çš„æ§åˆ¶å“åº”
#         self.robot_env = FrankyEnv(
#             action_space=action_space, 
#             inference_mode=True, 
#             robot_param=RobotParam(np.array([ 0.0, 0.0, -math.pi / 2]), np.array([ 0.53433071, 0.52905707, 0.00440881]))
#         )
        
#         logging.info(f"Trying to connect to policy server at {ip}:{port}...")
#         self.client = TCPClientPolicy(host=ip, port=port)
#         logging.info(f"Connected to policy server at {ip}:{port}.")
        
#         # Camera åˆå§‹åŒ–
#         from cameras.realsense_env import RealSenseEnv
        
#         # åªå¯åŠ¨æ‰‹è…•ç›¸æœº (Wrist-Only Inference)
#         self.wrist_camera = RealSenseEnv(camera_name="wrist_image", serial_number="342222072092", width=1280, height=720)
#         self.gripper_status = {"current_state": 0, "target_state": 0}
#         self.stop_evaluation = threading.Event()

#     def run(self, show_image: bool = False, task_name: str = "default_task"):
#         self.wrist_camera.start_monitoring()
        
#         logging.info("Waiting 2.0s for cameras to warm up...")
#         time.sleep(2.0)
        
#         logging.info("Starting inference loop...")
        
#         # =========================================================
#         # ğŸ”§ æ ¸å¿ƒå‚æ•°è°ƒä¼˜
#         # 1. EXECUTION_HORIZON: è®¾ä¸º 64 (ä¸æ¨¡å‹é¢„æµ‹é•¿åº¦ä¸€è‡´)
#         #    è¿™èƒ½å½»åº•æ¶ˆé™¤åœ¨ç›®æ ‡é™„è¿‘çš„"çŠ¹è±«"å’Œ"åå¤æ¨ªè·³"ã€‚
#         # 2. CONTROL_FREQUENCY: 25Hz (æ¯æ­¥ 0.04s)
#         # 3. MAX_STEP_RAD: å…³èŠ‚åŠ¨ä½œé™å¹…ï¼Œé˜²æ­¢å‰§çƒˆæŠ–åŠ¨
#         # =========================================================
#         EXECUTION_HORIZON = 64
#         CONTROL_FREQUENCY = 25  
#         STEP_DURATION = 1.0 / CONTROL_FREQUENCY # 0.04s
        
#         # 0.05 å¼§åº¦ â‰ˆ 2.8åº¦ã€‚é™åˆ¶æ¯ 0.04s æœ€å¤šè½¬è¿™ä¹ˆå¤§è§’åº¦ï¼Œé˜²æ­¢æŠ½æã€‚
#         MAX_STEP_RAD = 0.05 
        
#         last_executed_joints = None

#         while not self.stop_evaluation.is_set():
#             t0 = time.time()
            
#             # 1. è·å–å›¾åƒ
#             wrist_frame_data = self.wrist_camera.get_latest_frame()
#             if wrist_frame_data is None:
#                 time.sleep(0.01)
#                 continue
            
#             wrist_image = wrist_frame_data['bgr']
#             # æ„é€ å…¨é»‘ä¸»æ‘„å ä½ç¬¦ (é€‚é…è®­ç»ƒæ—¶çš„ Modality Dropout)
#             main_image = np.zeros_like(wrist_image)

#             # 2. è·å–çŠ¶æ€
#             joint_angles = self.robot_env.get_position(action_space=ActionSpace.JOINT_ANGLES)
#             gripper_width = self.robot_env.get_gripper_width()
#             eef_pose = self.robot_env.get_position(action_space=ActionSpace.EEF_POSE)
            
#             # æ„é€  8 ç»´ qpos (7å…³èŠ‚ + 1å¤¹çˆª)
#             qpos_8d = list(joint_angles) + [float(gripper_width)]
#             # æ„é€  State
#             state = np.concatenate([eef_pose, [gripper_width]])
            
#             # 3. æ„é€ è¯·æ±‚
#             element = {
#                 "observation/agentview_image": main_image, 
#                 "observation/wrist_image": wrist_image,
#                 "observation/state": state,
#                 "qpos": qpos_8d, 
#                 "prompt": task_name,
#             }

#             # 4. æ¨ç† (Blocking)
#             inference_results = self.client.infer(element)
            
#             if inference_results and "actions" in inference_results:
#                 new_actions = inference_results["actions"][0] # [64, 8]
                
#                 # å¥å£®æ€§æ£€æŸ¥
#                 if not isinstance(new_actions, list) or len(new_actions) == 0:
#                     continue

#                 # æˆªå–è¦æ‰§è¡Œçš„ç‰‡æ®µ (å…¨é‡æ‰§è¡Œä»¥æ¶ˆé™¤çŠ¹è±«)
#                 actions_to_execute = new_actions[:EXECUTION_HORIZON]
                
#                 print(f"  >>> Executing chunk ({len(actions_to_execute)} steps)...")

#                 for i, action in enumerate(actions_to_execute):
#                     # ç±»å‹æ£€æŸ¥
#                     if not isinstance(action, (list, tuple, np.ndarray)):
#                         continue

#                     # [å…³é”® 1] å¼ºåˆ¶è½¬ä¸º float64ï¼Œæ»¡è¶³ C++ æ¥å£è¦æ±‚
#                     action_np = np.array(action, dtype=np.float64)

#                     # [å…³é”® 2] ç©ºåŠ¨ä½œ/éæ³•å€¼æ‹¦æˆª
#                     if np.all(action_np == 0) or np.isnan(action_np).any():
#                         print(f"\râš ï¸ Invalid action detected, skipping chunk.", end="")
#                         break 
                    
#                     target_joints = action_np[:-1] # å‰7ä½ (Joints)
#                     gripper_val = action_np[-1]    # ç¬¬8ä½ (Gripper)

#                     # [å…³é”® 3] å¹³æ»‘é™å¹…é€»è¾‘ (Anti-Jitter)
#                     if last_executed_joints is not None:
#                         # è®¡ç®—å·®å€¼
#                         diff = target_joints - last_executed_joints
#                         # é™åˆ¶æœ€å¤§å˜åŒ–é‡ (Clip)
#                         diff_clipped = np.clip(diff, -MAX_STEP_RAD, MAX_STEP_RAD)
#                         # åº”ç”¨é™åˆ¶åçš„æ–°ç›®æ ‡
#                         target_joints = last_executed_joints + diff_clipped
                    
#                     # æ›´æ–°è®°å½•
#                     last_executed_joints = target_joints.copy()

#                     t_step_start = time.time()
                    
#                     # A. å…³èŠ‚æ§åˆ¶ (å¼‚æ­¥æ‰§è¡Œ)
#                     self.robot_env.step(target_joints, asynchronous=True)
                    
#                     # B. å¤¹çˆªæ§åˆ¶ (å¸¦çŠ¶æ€æœº)
#                     if gripper_val > 0.06: 
#                          if self.gripper_status["current_state"] != -1:
#                              self.robot_env.open_gripper(asynchronous=True)
#                              self.gripper_status["current_state"] = -1
#                     elif gripper_val < 0.02:
#                          if self.gripper_status["current_state"] != 1:
#                              self.robot_env.close_gripper(asynchronous=True)
#                              self.gripper_status["current_state"] = 1
                    
#                     # C. é¢‘ç‡æ§åˆ¶ (25Hz)
#                     dt = time.time() - t_step_start
#                     remain = STEP_DURATION - dt
#                     if remain > 0: 
#                         time.sleep(remain)

#             # å¯è§†åŒ– (ä»…åœ¨æ¨ç†é—´éš™åˆ·æ–°ï¼Œé¿å…é˜»å¡æ§åˆ¶å¾ªç¯)
#             if show_image:
#                 cv2.imshow("Wrist View", wrist_image)
#                 cv2.waitKey(1)

#             latency = (time.time() - t0) * 1000
#             print(f"\rChunk Latency: {latency:.1f}ms", end="")

#     def stop(self):
#         self.stop_evaluation.set()
#         time.sleep(0.5)
#         logging.info("System stopped.")

# if __name__ == "__main__":
#     logging.basicConfig(level=logging.INFO)
#     system = RobotPolicySystem(
#         action_space=ActionSpace.JOINT_ANGLES, 
#         ip="127.0.0.1", 
#         port=6000
#     )
#     try:
#         system.run(show_image=True, task_name="pick up the paper cup")
#     except KeyboardInterrupt:
#         system.stop()

import sys
import os
import time
import logging
import cv2
import numpy as np
import math
import threading
from collections import deque
from common.constants import ActionSpace
from robots.franky_env import FrankyEnv
from robots.robot_param import RobotParam
from systems.tcp_client import TCPClientPolicy 

# å¼•å…¥ä½ çš„ç›¸æœºåº“
from cameras.realsense_env import RealSenseEnv

class ImageRecorder(threading.Thread):
    def __init__(self, camera, buffer_size=16):
        super().__init__()
        self.camera = camera
        self.buffer_size = buffer_size
        self.running = False
        self.lock = threading.Lock()
        
        # ä¸¤ä¸ª Bufferï¼š
        # 1. raw_buffer: å­˜åŸå§‹å›¾ï¼Œç”¨äºæ˜¾ç¤º
        # 2. video_buffer: å­˜å¤„ç†åçš„ tensor/numpyï¼Œç”¨äºæ¨ç†
        self.latest_frame = None
        
        # è¿™é‡Œçš„ buffer åªè¦å­˜ numpy æ•°ç»„å³å¯ï¼Œä¸éœ€è¦å­˜ Tensorï¼Œ
        # è½¬æ¢ Tensor çš„å·¥ä½œäº¤ç»™ Server ç«¯ï¼Œæˆ–è€…åœ¨å‘é€å‰åšï¼Œå‡å°‘ä¼ è¾“å‹åŠ›
        # ä½†ä¸ºäº†é…åˆä½ çš„ Server é€»è¾‘ï¼Œæˆ‘ä»¬è¿™é‡Œåªå­˜åŸå§‹ BGR å›¾åƒ
        self.frame_buffer = deque(maxlen=buffer_size) 
        self.stop_event = threading.Event()

    def run(self):
        self.running = True
        self.camera.start_monitoring()
        logging.info("[ImageRecorder] Background thread started.")
        
        while not self.stop_event.is_set():
            # è·å–æœ€æ–°å¸§ (è¿™æ˜¯è½»é‡çº§æ“ä½œ)
            data = self.camera.get_latest_frame()
            if data is not None:
                img = data['bgr']
                
                with self.lock:
                    self.latest_frame = img.copy()
                    # å­˜å…¥ Buffer
                    self.frame_buffer.append(img)
                
                # å®æ—¶æ˜¾ç¤º (åœ¨è¿™é‡Œæ˜¾ç¤ºæœ€æµç•…)
                cv2.imshow("Wrist View (Real-time)", img)
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    self.stop_event.set()
            
            # ä¿æŒçº¦ 30Hz çš„é‡‡æ ·ç‡ (æ ¹æ®ä½ è®­ç»ƒæ•°æ®çš„å¸§ç‡è°ƒæ•´)
            # å¦‚æœä½ è®­ç»ƒæ˜¯ 10Hzï¼Œè¿™é‡Œæ”¹æˆ time.sleep(0.1)
            time.sleep(0.033) 
        
        cv2.destroyAllWindows()
        logging.info("[ImageRecorder] Stopped.")

    def get_inference_input(self):
        """
        è·å–ç”¨äºæ¨ç†çš„ snapshotã€‚
        å¦‚æœ Buffer è¿˜æ²¡æ»¡ï¼Œå°±ç”¨ç¬¬ä¸€å¸§å¤åˆ¶å¡«å…… (Padding)ã€‚
        """
        with self.lock:
            if len(self.frame_buffer) == 0:
                return None, None
            
            current_img = self.latest_frame.copy()
            
            # æ‹¿åˆ° Buffer çš„å¿«ç…§
            frames_snapshot = list(self.frame_buffer)
        
        # ç­–ç•¥ï¼šå¦‚æœä¸å¤Ÿ 16 å¸§ï¼Œç”¨ç¬¬ä¸€å¸§è¡¥é½å¤´éƒ¨ (Padding Head)
        # è¿™æ ·ä¿è¯æ—¶åºç›¸å¯¹å…³ç³»æ˜¯æ­£ç¡®çš„
        while len(frames_snapshot) < self.buffer_size:
            frames_snapshot.insert(0, frames_snapshot[0])
            
        
        return current_img

    def stop(self):
        self.stop_event.set()
        self.join()

class RobotPolicySystem:
    def __init__(self, action_space: ActionSpace = ActionSpace.JOINT_ANGLES, ip: str = "127.0.0.1", port: int = 6000):
        self.action_space = action_space
        
        # Robot
        self.robot_env = FrankyEnv(
            action_space=action_space, 
            inference_mode=True, 
            robot_param=RobotParam(np.array([ 0.0, 0.0, -math.pi / 2]), np.array([ 0.53433071, 0.52905707, 0.00440881]))
        )
        
        # Client
        logging.info(f"Connecting to {ip}:{port}...")
        self.client = TCPClientPolicy(host=ip, port=port)
        logging.info("Connected.")
        
        # Camera & Recorder
        self.wrist_camera = RealSenseEnv(camera_name="wrist_image", serial_number="342222072092", width=1280, height=720)
        # å¯åŠ¨åå°é‡‡é›†çº¿ç¨‹
        self.recorder = ImageRecorder(self.wrist_camera, buffer_size=16)
        
        self.gripper_status = {"current_state": 0}
        self.stop_evaluation = threading.Event()

    def run(self, task_name: str = "default_task"):
        # å¯åŠ¨åå°é‡‡é›†
        self.recorder.start()
        
        logging.info("Waiting 2.0s for warmup...")
        time.sleep(2.0)
        
        # å‚æ•°è®¾ç½®
        EXECUTION_HORIZON = 15  # ä¿¡ä»»æ¨¡å‹ï¼Œåšå®Œ 15 æ­¥
        MAX_STEP_RAD = 0.05     # é™å¹…
        last_executed_joints = None
        
        logging.info("Starting inference loop...")

        try:
            while not self.stop_evaluation.is_set():
                if not self.recorder.is_alive():
                    break

                t0 = time.time()
                
                # 1. ä»åå°çº¿ç¨‹æ‹¿ã€æœ€æ–°é²œã€‘çš„ä¸€å¼ å›¾
                # å³ä½¿ä¸»çº¿ç¨‹å¡äº† 5 ç§’ï¼Œè¿™é‡Œæ‹¿åˆ°çš„ä¹Ÿæ˜¯ 0.001 ç§’å‰ç›¸æœºåˆšæ‹åˆ°çš„
                wrist_image = self.recorder.get_inference_input()
                
                if wrist_image is None:
                    time.sleep(0.01)
                    continue

                # 2. è·å–æœºå™¨äººçŠ¶æ€
                joint_angles = self.robot_env.get_position(action_space=ActionSpace.JOINT_ANGLES)
                gripper_width = self.robot_env.get_gripper_width()
                eef_pose = self.robot_env.get_position(action_space=ActionSpace.EEF_POSE)
                
                qpos_8d = list(joint_angles) + [float(gripper_width)]
                state = np.concatenate([eef_pose, [gripper_width]])
                
                # 3. å‘é€è¯·æ±‚
                # æ³¨æ„ï¼šæˆ‘ä»¬åœ¨ Server ç«¯å·²ç»æ”¹æˆäº† "æ”¶åˆ°ä¸€å¼ å›¾ -> å¤åˆ¶å¡«æ»¡ Buffer" çš„é™æ€å›¾ç­–ç•¥
                # è¿™é…åˆè¿™é‡Œ "è·å–æœ€æ–°é²œçš„ä¸€å¼ å›¾" æ˜¯ç›®å‰æœ€ç¨³å¥çš„ç»„åˆ
                element = {
                    "observation/agentview_image": np.zeros_like(wrist_image), 
                    "observation/wrist_image": wrist_image,
                    "observation/state": state,
                    "qpos": qpos_8d, 
                    "prompt": task_name,
                }

                # 4. æ¨ç† (Blocking 2.5s)
                inference_results = self.client.infer(element)
                
                if inference_results and "actions" in inference_results:
                    new_actions = inference_results["actions"][0]
                    
                    if not isinstance(new_actions, list) or len(new_actions) == 0:
                        continue

                    # æ‰§è¡Œ 15 æ­¥
                    actions_to_execute = new_actions[:EXECUTION_HORIZON]
                    
                    print(f"  >>> Executing chunk ({len(actions_to_execute)} steps)...")

                    for action in actions_to_execute:
                        if not isinstance(action, (list, tuple, np.ndarray)): continue
                        
                        # æ•°æ®å¤„ç†
                        action_np = np.array(action, dtype=np.float64)
                        if np.all(action_np == 0) or np.isnan(action_np).any(): break
                        
                        target_joints = action_np[:-1]
                        gripper_val = action_np[-1]

                        # å¹³æ»‘é™å¹…
                        if last_executed_joints is not None:
                            diff = np.clip(target_joints - last_executed_joints, -MAX_STEP_RAD, MAX_STEP_RAD)
                            target_joints = last_executed_joints + diff
                        
                        last_executed_joints = target_joints.copy()

                        # æ‰§è¡Œ
                        t_step_start = time.time()
                        self.robot_env.step(target_joints, asynchronous=True)
                        
                        # å¤¹çˆª
                        if gripper_val > 0.06 and self.gripper_status["current_state"] != -1:
                             self.robot_env.open_gripper(asynchronous=True)
                             self.gripper_status["current_state"] = -1
                        elif gripper_val < 0.02 and self.gripper_status["current_state"] != 1:
                             self.robot_env.close_gripper(asynchronous=True)
                             self.gripper_status["current_state"] = 1
                        
                        # æ§é¢‘ 25Hz
                        remain = 0.04 - (time.time() - t_step_start)
                        if remain > 0: time.sleep(remain)

                latency = (time.time() - t0) * 1000
                print(f"\rLoop Latency: {latency:.1f}ms", end="")

        except KeyboardInterrupt:
            pass
        finally:
            self.stop()

    def stop(self):
        self.stop_evaluation.set()
        self.recorder.stop()
        time.sleep(0.5)
        logging.info("System stopped.")

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    system = RobotPolicySystem(ip="127.0.0.1", port=6000)
    system.run(task_name="pick up the paper cup")